{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch as t\nimport torch.nn as nn\nt.manual_seed(3407)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:00:30.953898Z","iopub.execute_input":"2025-12-25T19:00:30.954694Z","iopub.status.idle":"2025-12-25T19:00:30.961329Z","shell.execute_reply.started":"2025-12-25T19:00:30.954657Z","shell.execute_reply":"2025-12-25T19:00:30.960650Z"}},"outputs":[{"execution_count":116,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x78dc59378470>"},"metadata":{}}],"execution_count":116},{"cell_type":"markdown","source":"# Creating dataset\n### Dataset will be of size 19968*7 we will rearrange it to 312 * 64 * 7\n","metadata":{}},{"cell_type":"code","source":"Data = t.randint(0, 9, (19968,7))\nData = Data.reshape(312, 64, 7)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:00:30.962872Z","iopub.execute_input":"2025-12-25T19:00:30.963111Z","iopub.status.idle":"2025-12-25T19:00:30.979124Z","shell.execute_reply.started":"2025-12-25T19:00:30.963080Z","shell.execute_reply":"2025-12-25T19:00:30.978043Z"}},"outputs":[],"execution_count":117},{"cell_type":"code","source":"Data.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:00:30.980223Z","iopub.execute_input":"2025-12-25T19:00:30.980618Z","iopub.status.idle":"2025-12-25T19:00:30.993571Z","shell.execute_reply.started":"2025-12-25T19:00:30.980579Z","shell.execute_reply":"2025-12-25T19:00:30.992826Z"}},"outputs":[{"execution_count":118,"output_type":"execute_result","data":{"text/plain":"torch.Size([312, 64, 7])"},"metadata":{}}],"execution_count":118},{"cell_type":"code","source":"train_X = Data[:250,:,:]\ntest_X = Data[251:,:,:]\ntrain_X.shape, test_X.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:00:30.995503Z","iopub.execute_input":"2025-12-25T19:00:30.995830Z","iopub.status.idle":"2025-12-25T19:00:31.012750Z","shell.execute_reply.started":"2025-12-25T19:00:30.995801Z","shell.execute_reply":"2025-12-25T19:00:31.011893Z"}},"outputs":[{"execution_count":119,"output_type":"execute_result","data":{"text/plain":"(torch.Size([250, 64, 7]), torch.Size([61, 64, 7]))"},"metadata":{}}],"execution_count":119},{"cell_type":"code","source":"sorted_y = t.sort(Data).values\nreverse_y = t.flip(Data, [2])\nsorted_y.shape, reverse_y.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:00:31.013822Z","iopub.execute_input":"2025-12-25T19:00:31.014088Z","iopub.status.idle":"2025-12-25T19:00:31.031583Z","shell.execute_reply.started":"2025-12-25T19:00:31.014063Z","shell.execute_reply":"2025-12-25T19:00:31.030825Z"}},"outputs":[{"execution_count":120,"output_type":"execute_result","data":{"text/plain":"(torch.Size([312, 64, 7]), torch.Size([312, 64, 7]))"},"metadata":{}}],"execution_count":120},{"cell_type":"code","source":"Y_DATA = t.cat((sorted_y, reverse_y), 2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:00:31.032661Z","iopub.execute_input":"2025-12-25T19:00:31.032936Z","iopub.status.idle":"2025-12-25T19:00:31.045741Z","shell.execute_reply.started":"2025-12-25T19:00:31.032910Z","shell.execute_reply":"2025-12-25T19:00:31.044863Z"}},"outputs":[],"execution_count":121},{"cell_type":"code","source":"train_y = Y_DATA[:250,:]\ntest_y = Y_DATA[251:,:] ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:00:31.046808Z","iopub.execute_input":"2025-12-25T19:00:31.047126Z","iopub.status.idle":"2025-12-25T19:00:31.054077Z","shell.execute_reply.started":"2025-12-25T19:00:31.047099Z","shell.execute_reply":"2025-12-25T19:00:31.053335Z"}},"outputs":[],"execution_count":122},{"cell_type":"markdown","source":"# Model Building","metadata":{}},{"cell_type":"code","source":"class Embedding(nn.Module):\n    def __init__(self,embedding_dims):\n        super(Embedding, self).__init__()\n        self.emb = nn.Linear(1,embedding_dims)\n    def forward(self, x):\n        x = x.float() \n        x = x.unsqueeze(-1)\n        return self.emb(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:00:31.055014Z","iopub.execute_input":"2025-12-25T19:00:31.055393Z","iopub.status.idle":"2025-12-25T19:00:31.070232Z","shell.execute_reply.started":"2025-12-25T19:00:31.055367Z","shell.execute_reply":"2025-12-25T19:00:31.069374Z"}},"outputs":[],"execution_count":123},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, embedding_dims=512, heads=1, mask = False):\n        super(MultiHeadAttention, self).__init__()\n        self.embedding_dims = embedding_dims\n        self.heads = heads\n        self.feature_map = embedding_dims//heads\n        self.mask = mask\n        self.Wq = nn.Parameter(t.randn(embedding_dims, embedding_dims, dtype = t.float32)) # parameter class makes it learnable\n        self.Wk = nn.Parameter(t.randn(embedding_dims, embedding_dims, dtype = t.float32)) # parameter class makes it learnable\n        self.Wv = nn.Parameter(t.randn(embedding_dims, embedding_dims, dtype = t.float32)) # parameter class makes it learnable\n    def forward(self,X:t.Tensor, enc_output):\n        self.Xshape = X.shape\n        # print(self.Xshape)\n        self.Xq = X.clone()\n        self.Xk = enc_output if enc_output is not None else X.clone()\n        self.Xv = enc_output if enc_output is not None else X.clone()\n        self.Q = t.matmul(self.Xq,self.Wq)\n        self.K = t.matmul(self.Xk,self.Wk)\n        self.V = t.matmul(self.Xv,self.Wv)\n        # concatination\n        self.Q = self.Q.view(self.Xshape[0], self.Xshape[1], self.heads, self.feature_map).transpose(1,2)\n        self.K = self.K.view(self.Xshape[0], self.Xshape[1], self.heads, self.feature_map).transpose(1,2)\n        self.V = self.V.view(self.Xshape[0], self.Xshape[1], self.heads, self.feature_map).transpose(1,2)\n        return self.product()\n    def product(self):\n        product = t.matmul(self.Q, self.K.transpose(-2,-1))\n        scale = t.sqrt(t.tensor(self.embedding_dims//self.heads))\n        product = product/scale\n        if self.mask:\n            tril = t.tril(t.ones(self.Xshape[1], self.Xshape[1]))\n            product = product.masked_fill(tril == 0, float('-inf'))\n        product = t.nn.functional.softmax(product, dim=-1)\n        contextual_embedding = t.matmul(product, self.V)\n        contextual_embedding = contextual_embedding.transpose(1, 2).contiguous()\n        output = contextual_embedding.view(self.Xshape[0], self.Xshape[1], self.embedding_dims)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:00:31.140103Z","iopub.execute_input":"2025-12-25T19:00:31.140640Z","iopub.status.idle":"2025-12-25T19:00:31.152823Z","shell.execute_reply.started":"2025-12-25T19:00:31.140611Z","shell.execute_reply":"2025-12-25T19:00:31.152047Z"}},"outputs":[],"execution_count":124},{"cell_type":"code","source":"#Self wrote\nclass PositionalEncoding():\n    def __init__(self, embedding_dims = 512, context = 512):\n        self.embedding_size = embedding_dims\n        self.context = context\n        self.matrix = t.zeros(self.context, self.embedding_size, dtype = t.float32)\n        for pos in range(self.context):\n            for i in range(self.embedding_size):\n                if(i%2==0):\n                    power = i/self.embedding_size\n                    inner = pos/(10000**power)\n                    self.matrix[pos][i] = t.sin(inner)\n                else:\n                    power = (i-1)/self.embedding_size\n                    inner = pos/(10000**power)\n                    self.matrix[pos][i]=t.cos(inner)\n    def forward(self, x):\n        return x+self.matrix[:x.shape[1], :]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:00:31.154171Z","iopub.execute_input":"2025-12-25T19:00:31.154442Z","iopub.status.idle":"2025-12-25T19:00:31.174000Z","shell.execute_reply.started":"2025-12-25T19:00:31.154417Z","shell.execute_reply":"2025-12-25T19:00:31.172909Z"}},"outputs":[],"execution_count":125},{"cell_type":"code","source":"#Ai Generated\nimport torch\nimport torch.nn as nn\nimport math\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, embedding_dims=512, context=512):\n        super().__init__()\n\n        pe = torch.zeros(context, embedding_dims)\n        position = torch.arange(0, context, dtype=torch.float32).unsqueeze(1)\n\n        div_term = torch.exp(\n            torch.arange(0, embedding_dims, 2, dtype=torch.float32)\n            * (-math.log(10000.0) / embedding_dims)\n        )\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        pe = pe.unsqueeze(0)  # shape: (1, context, embedding_dims)\n        self.register_buffer(\"pe\", pe)\n\n    def forward(self, x):\n        # x shape: (batch, seq_len, embedding_dims)\n        # print(\"positional_shape: \",self.pe.shape)\n        # print(\"x_shape: \", x.shape)\n        seq_len = x.size(1)\n        return x + self.pe[:, :seq_len, :]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:00:31.175044Z","iopub.execute_input":"2025-12-25T19:00:31.175333Z","iopub.status.idle":"2025-12-25T19:00:31.190171Z","shell.execute_reply.started":"2025-12-25T19:00:31.175298Z","shell.execute_reply":"2025-12-25T19:00:31.189232Z"}},"outputs":[],"execution_count":126},{"cell_type":"code","source":"class FeedForwardNN(nn.Module):\n    def __init__(self, embedding_dims):\n        super(FeedForwardNN,self).__init__()\n        self.z1 = nn.Linear(embedding_dims,512)\n        self.z2 = nn.Linear(512,embedding_dims)\n    def forward(self, x):\n        x = self.z1(x)\n        x = nn.ReLU()(x)\n        x = self.z2(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:00:31.191208Z","iopub.execute_input":"2025-12-25T19:00:31.191597Z","iopub.status.idle":"2025-12-25T19:00:31.208468Z","shell.execute_reply.started":"2025-12-25T19:00:31.191561Z","shell.execute_reply":"2025-12-25T19:00:31.207361Z"}},"outputs":[],"execution_count":127},{"cell_type":"code","source":"d_model = 12\nheads = 8\ncontext = 7\nclass Encoder(nn.Module):\n    def __init__(self, d_model, heads, context, first=False):\n        super(Encoder, self).__init__()\n        self.first = first\n        self.embedding = Embedding(d_model)\n        self.pos = PositionalEncoding(embedding_dims = d_model, context = 7)\n        self.attention = MultiHeadAttention(embedding_dims=d_model, heads=heads, mask = False)\n        self.feedforward = FeedForwardNN(embedding_dims=d_model)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n    def forward(self, x):\n        if(self.first):\n            x = self.pos(self.embedding(x))\n            # print(f\"DEBUG - After Embedding: {x.shape}\")\n        # print(f\"DEBUG - After Positional: {x.shape}\")\n        contextual_embedding = self.attention(x, None)\n        # print(f\"DEBUG - After Attention: {contextual_embedding.shape}\")\n        add = x + contextual_embedding\n        norm = self.norm1(add)\n        z = self.feedforward(norm)\n        z = z + norm\n        z = self.norm2(z)\n        return z","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:00:31.210742Z","iopub.execute_input":"2025-12-25T19:00:31.211017Z","iopub.status.idle":"2025-12-25T19:00:31.223003Z","shell.execute_reply.started":"2025-12-25T19:00:31.210990Z","shell.execute_reply":"2025-12-25T19:00:31.222131Z"}},"outputs":[],"execution_count":128},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, d_model, heads, context, first=False):\n        super(Decoder, self).__init__()\n        self.first = first\n        self.embedding = Embedding(d_model)\n        self.pos = PositionalEncoding(embedding_dims=d_model, context=context)\n        self.masked_attention = MultiHeadAttention(embedding_dims=d_model, heads=heads, mask=True)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.cross_attention = MultiHeadAttention(embedding_dims=d_model, heads=heads, mask=False)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.feedforward = FeedForwardNN(embedding_dims=d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n\n    def forward(self, x, encoder_output):\n        if(self.first):\n            x = self.pos(self.embedding(x))\n        \n        attn1_out = self.masked_attention(x, encoder_output)\n        x = self.norm1(x + attn1_out)\n        attn2_out = self.cross_attention_forward(x, encoder_output)\n        x = self.norm2(x + attn2_out)\n        ff_out = self.feedforward(x)\n        z = self.norm3(x + ff_out)\n        return z\n\n    def cross_attention_forward(self, x, enc_out):\n        return self.cross_attention(x, enc_out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:00:31.224036Z","iopub.execute_input":"2025-12-25T19:00:31.224620Z","iopub.status.idle":"2025-12-25T19:00:31.244446Z","shell.execute_reply.started":"2025-12-25T19:00:31.224576Z","shell.execute_reply":"2025-12-25T19:00:31.243657Z"}},"outputs":[],"execution_count":129},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass prediction_layer(nn.Module):\n    def __init__(self, input_dims):\n        super(prediction_layer, self).__init__()\n        self.linear = nn.Linear(input_dims, 140)\n\n    def forward(self, x):\n        raw_output = self.linear(x) \n        logits = raw_output.view(-1, 14, 10)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:00:31.245500Z","iopub.execute_input":"2025-12-25T19:00:31.245815Z","iopub.status.idle":"2025-12-25T19:00:31.261736Z","shell.execute_reply.started":"2025-12-25T19:00:31.245778Z","shell.execute_reply":"2025-12-25T19:00:31.260723Z"}},"outputs":[],"execution_count":130},{"cell_type":"code","source":"class TransformerNN(nn.Module):\n    def __init__(self):\n        super(TransformerNN, self).__init__()\n        self.en1 = Encoder(12, 1, 7, True)\n        self.en2 = Encoder(12, 1, 7)\n        self.en3 = Encoder(12, 1, 7)\n        self.en4 = Encoder(12, 1, 7)\n        self.en5 = Encoder(12, 1, 7)\n        self.en6 = Encoder(12, 1, 7)\n        self.en7 = Encoder(12, 1, 7)\n        self.de1 = Decoder(12, 1, 7, True)\n        self.de2 = Decoder(12, 1, 7)\n        self.de3 = Decoder(12, 1, 7)\n        self.de4 = Decoder(12, 1, 7)\n        self.de5 = Decoder(12, 1, 7)\n        self.de6 = Decoder(12, 1, 7)\n        self.de7 = Decoder(12, 1, 7)\n        self.pred_layer = prediction_layer(84)\n    def forward(self, x):\n        # enc_out = x\n        enc_out = self.en1(x)\n        # print('shape: ', x.shape)\n        enc_out = self.en2(enc_out)\n        enc_out = self.en3(enc_out)\n        enc_out = self.en4(enc_out)\n        enc_out = self.en5(enc_out)\n        enc_out = self.en6(enc_out)\n        enc_out = self.en7(enc_out)\n        # print('enc_out.shape: ',enc_out.shape)\n        dec_out = self.de1(x=x, encoder_output=enc_out)\n        # print(\"de1: \", dec_out.shape)\n        dec_out = self.de2(dec_out, enc_out)\n        dec_out = self.de3(dec_out, enc_out)\n        dec_out = self.de4(dec_out, enc_out)\n        dec_out = self.de5(dec_out, enc_out)\n        dec_out = self.de6(dec_out, enc_out)\n        dec_out = self.de7(dec_out, enc_out)\n        # print(\"decoder_ouput: \", dec_out.shape)\n        dec_out = dec_out.reshape(64,-1)\n        y_pred = self.pred_layer(dec_out)\n        return y_pred","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:00:31.262808Z","iopub.execute_input":"2025-12-25T19:00:31.263076Z","iopub.status.idle":"2025-12-25T19:00:31.275398Z","shell.execute_reply.started":"2025-12-25T19:00:31.263048Z","shell.execute_reply":"2025-12-25T19:00:31.274531Z"}},"outputs":[],"execution_count":131},{"cell_type":"markdown","source":"# Training\n","metadata":{}},{"cell_type":"code","source":"model = TransformerNN()\ncriterion = nn.CrossEntropyLoss()\noptimizer = t.optim.Adam(model.parameters(), 0.0001)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:00:31.276525Z","iopub.execute_input":"2025-12-25T19:00:31.276944Z","iopub.status.idle":"2025-12-25T19:00:31.308475Z","shell.execute_reply.started":"2025-12-25T19:00:31.276905Z","shell.execute_reply":"2025-12-25T19:00:31.307647Z"}},"outputs":[],"execution_count":132},{"cell_type":"code","source":"# epochs = 10\n# for epoch in range(epochs):\n#     for x,y in zip(train_X,train_y):\n#         # x = x.to(dtype=t.float32)\n#         # y = y.to(dtype=t.float32)\n#         # print(x.shape)\n#         logits = model(x)\n#         # print(logits.shape)\n#         # print(y.shape)\n#         logits = logits.view(-1,10)\n#         # print(\"logits type:- \", logits.dtype)\n#         targets = y.view(-1)\n#         # print(\"targets type:- \", targets.dtype)\n#         loss = criterion(logits, targets)\n#         # optimizer.backward()\n#         model.zero_grad()\n#         loss.backward()\n#         optimizer.step()\n#     print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, 10, loss.item()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:00:31.309473Z","iopub.execute_input":"2025-12-25T19:00:31.309725Z","iopub.status.idle":"2025-12-25T19:00:31.313509Z","shell.execute_reply.started":"2025-12-25T19:00:31.309701Z","shell.execute_reply":"2025-12-25T19:00:31.312695Z"}},"outputs":[],"execution_count":133},{"cell_type":"code","source":"epochs = 10\nfor epoch in range(epochs):\n    running_loss = 0.0  # <--- Step 1: Initialize a counter\n    \n    for x, y in zip(train_X, train_y):\n        optimizer.zero_grad()        # <--- Standard practice: zero first\n        \n        logits = model(x).view(-1, 10)\n        targets = y.view(-1)\n        \n        loss = criterion(logits, targets)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()  # <--- Step 2: Add up the loss\n        \n    # Step 3: Calculate average (Divide by total number of items)\n    avg_loss = running_loss / len(train_X)\n    print('Epoch [{}/{}], Avg Loss: {:.4f}'.format(epoch+1, 10, avg_loss))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:00:31.314570Z","iopub.execute_input":"2025-12-25T19:00:31.314954Z","iopub.status.idle":"2025-12-25T19:03:01.057292Z","shell.execute_reply.started":"2025-12-25T19:00:31.314927Z","shell.execute_reply":"2025-12-25T19:03:01.056365Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/10], Avg Loss: 2.0159\nEpoch [2/10], Avg Loss: 1.7448\nEpoch [3/10], Avg Loss: 1.6436\nEpoch [4/10], Avg Loss: 1.5699\nEpoch [5/10], Avg Loss: 1.5092\nEpoch [6/10], Avg Loss: 1.4577\nEpoch [7/10], Avg Loss: 1.4078\nEpoch [8/10], Avg Loss: 1.3718\nEpoch [9/10], Avg Loss: 1.3391\nEpoch [10/10], Avg Loss: 1.3156\n","output_type":"stream"}],"execution_count":134},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\nprint(f\"Total parameters: {total_params}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:03:01.058559Z","iopub.execute_input":"2025-12-25T19:03:01.058822Z","iopub.status.idle":"2025-12-25T19:03:01.064137Z","shell.execute_reply.started":"2025-12-25T19:03:01.058797Z","shell.execute_reply":"2025-12-25T19:03:01.063245Z"}},"outputs":[{"name":"stdout","text":"Total parameters: 201516\n","output_type":"stream"}],"execution_count":135}]}